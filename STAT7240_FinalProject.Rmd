---
title: 'STAT 7240: project'
author: "Kelly Lavertu"
date: "October 18, 2023"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r initiatePackages, message=FALSE}
# Packages: you can add/change (these are suggestions)

library(tidyverse)
library(OpenImageR)
library(keras)
library(caret)
library(fastai)
library(magrittr)
library(tensorflow)
library(reticulate)
library(stringr)
library(doParallel)

library(xgboost)
library(SciServer)
library(DBI)
library(dplyr)
library(dbplyr)
library(odbc)
library(imager)
library(httr)
library(jpeg)


library(factoextra)
library(cluster)
library(tidyr)
library(ggplot2)
library(ggdendro)
library(dendextend)


```


```{r}

# set work directory, load all images

setwd("C:/Users/Kelly/Documents/appdatamining/projectimages")

path = "C:/Users/Kelly/Documents/appdatamining/projectimages"

filepathnames = get_image_files(path)

```


```{r}
filenameslist = list.files(recursive = T, pattern="\\.jpeg$",
    full.names=T)

labels <- sapply(strsplit(filenameslist, "/"), `[`, 2)  # Extract the subfolder name

preprocess_image <- function(image_path) {
  img <- load.image(image_path)
  as.numeric(img)  # Convert to numeric vector
}


features = do.call(rbind, lapply(filenameslist, preprocess_image))

image.data = data.frame(image = I(features), label = labels)
# this is for a deep learning model
# train.data.gen = image_data_generator(rescale = 1/255)
# 
# train.generator <- flow_images_from_directory(
#   directory = path,
#   generator = train.data.gen,
#   target_size = c(150, 150), # Adjust depending on your model's input size
#   batch_size = 32,
#   class_mode = 'categorical'
# )
# 
# train.generator$class_indices


```

```{r}
set.seed(12)

data.part = createDataPartition(image.data$label, p = 0.1, list = F)

trainingData = image.data[data.part, ]
# testingData = image.data[-data.part, ]


```


```{r}
# # for smaller sample sized training
# # random forest
# prop.table(table(image.data$label))
# 
# registerDoParallel(cores = detectCores()/2)
# # the proportions for each class are split pretty evenly so as a balanced dataset I will use accuracy
# 
# 
# my.grid = expand.grid(mtry = c(0, 2, 4, 6),
#                       splitrule = "gini",
#                       min.node.size = c(1, 3, 5
#                                         
#                                         ))
# set.seed(12)
# 
# rf = caret::train(label ~.,
#                   data = trainingData,
#                   method = "ranger",
#                   metric = "Accuracy",
#                   tuneGrid = my.grid,
#                   num.trees = 4,
#                   trControl = trainControl(method = "cv", number = 10, allowParallel = T))
# 


```

```{r}
# # Identify the best model
# rf$results %>% arrange(desc(Accuracy))
```

```{r}
# xgboost
# choosing learning rate as .3,.4 -- may have to change later -- did change
xgb.eta = c(0.2, .4)
# max_depth (default 6)
xgb.depth = c(3, 6)
# gamma (default of 0)
xgb.gamma = c(0, 5)
# colsample_bytree (default 1)
xgb.col = c(0.75, 1)
# min_child_weight (default 1)
xgb.child = c(1, 2)
# subsample (default 1)
xgb.sub = c(0.75, 1)
# nrounds (default 100)
xgb.num = c(100, 150)


xgb.grid = expand.grid(eta = xgb.eta,
                       max_depth = xgb.depth,
                       gamma = xgb.gamma,
                       colsample_bytree = xgb.col,
                       min_child_weight = xgb.child,
                       subsample = xgb.sub,
                       nrounds = xgb.num)
  
#   * Select an appropriate evaluation metric and explain your choice 
## I will use Kappa again for the same reason I used it in the previous question
xgb.metric = "Accuracy"


#   * Train the models (*Note*: set a random number seed of your choice prior to training)
set.seed(2)
xgb.tune = caret::train(label ~.,
                        data = trainingData,
                        method = "xgbTree",
                        metric = xgb.metric,
                        verbosity = 0,
                        trControl = trainControl(method = "cv", number = 10),
                        tuneGrid = xgb.grid
                        )


#   * Identify the best performing model based on your evaluation criteria
# 
#   eta max_depth gamma colsample_bytree min_child_weight subsample nrounds  Accuracy     Kappa       AccuracySD      KappaSD
# 1  0.2    3      5      0.75                  2           0.75      150   0.7820994   0.5107927     0.04811178    0.09864681

xgb.best = xgb.tune$results %>% arrange(desc(Accuracy))
```

